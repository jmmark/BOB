{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import pre_processing as pp\n",
    "import h5py\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get and process the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"all_gbv_lyrics.txt\"\n",
    "text = pp.load_n_scrub(path)\n",
    "verses = pp.overlapping_verses(text, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i want to start a new life <eol>', 'with my valuable hunting knife <eol>', 'she will shine like a new girl <eol>', 'and i want to shout out our love to the world hit it <eol>']\n"
     ]
    }
   ],
   "source": [
    "print(text[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text)\n",
    "encoded_text = tokenizer.texts_to_sequences(text)\n",
    "flat_encoded = [item for sublist in encoded_text for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_sequences = []\n",
    "for i in range(1, len(flat_encoded)):\n",
    "    one_word_sequences += [flat_encoded[i-1:i+1]]\n",
    "\n",
    "one_word_sequences = np.array(one_word_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_X, one_word_y = one_word_sequences[:,0], one_word_sequences[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_y = to_categorical(one_word_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now can build up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1, 50)             322500    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6450)              328950    \n",
      "=================================================================\n",
      "Total params: 671,650\n",
      "Trainable params: 671,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "one_word_model = Sequential()\n",
    "one_word_model.add(Embedding(vocab_size, 50, input_length = 1))\n",
    "one_word_model.add(LSTM(50))\n",
    "one_word_model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(one_word_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75721/75721 [==============================] - 20s 259us/step - loss: 3.7505 - acc: 0.2946\n",
      "Epoch 2/100\n",
      "75721/75721 [==============================] - 19s 245us/step - loss: 3.6962 - acc: 0.2955\n",
      "Epoch 3/100\n",
      "75721/75721 [==============================] - 19s 244us/step - loss: 3.6682 - acc: 0.29511s - loss\n",
      "Epoch 4/100\n",
      "75721/75721 [==============================] - 18s 244us/step - loss: 3.6462 - acc: 0.2943\n",
      "Epoch 5/100\n",
      "75721/75721 [==============================] - 19s 245us/step - loss: 3.6283 - acc: 0.2950\n",
      "Epoch 6/100\n",
      "75721/75721 [==============================] - 19s 245us/step - loss: 3.6140 - acc: 0.2941\n",
      "Epoch 7/100\n",
      "75721/75721 [==============================] - 19s 245us/step - loss: 3.6023 - acc: 0.2946\n",
      "Epoch 8/100\n",
      "75721/75721 [==============================] - 19s 245us/step - loss: 3.5926 - acc: 0.29430s - loss: 3.5923 - acc: 0.294\n",
      "Epoch 9/100\n",
      "75721/75721 [==============================] - 19s 245us/step - loss: 3.5842 - acc: 0.2936\n",
      "Epoch 10/100\n",
      "75721/75721 [==============================] - 19s 246us/step - loss: 3.5767 - acc: 0.29410s - loss: 3.5763 - acc: 0.\n",
      "Epoch 11/100\n",
      "75721/75721 [==============================] - 19s 246us/step - loss: 3.5703 - acc: 0.2932\n",
      "Epoch 12/100\n",
      "75721/75721 [==============================] - 19s 246us/step - loss: 3.5648 - acc: 0.2934\n",
      "Epoch 13/100\n",
      "75721/75721 [==============================] - 19s 246us/step - loss: 3.5592 - acc: 0.2932\n",
      "Epoch 14/100\n",
      "75721/75721 [==============================] - 19s 246us/step - loss: 3.5549 - acc: 0.2932\n",
      "Epoch 15/100\n",
      "75721/75721 [==============================] - 19s 246us/step - loss: 3.5499 - acc: 0.2940\n",
      "Epoch 16/100\n",
      "75721/75721 [==============================] - 19s 247us/step - loss: 3.5462 - acc: 0.2940\n",
      "Epoch 17/100\n",
      "75721/75721 [==============================] - 19s 247us/step - loss: 3.5419 - acc: 0.2934\n",
      "Epoch 18/100\n",
      "75721/75721 [==============================] - 19s 248us/step - loss: 3.5381 - acc: 0.2936\n",
      "Epoch 19/100\n",
      "75721/75721 [==============================] - 19s 248us/step - loss: 3.5347 - acc: 0.2933\n",
      "Epoch 20/100\n",
      "75721/75721 [==============================] - 19s 247us/step - loss: 3.5318 - acc: 0.2931\n",
      "Epoch 21/100\n",
      "75721/75721 [==============================] - 19s 247us/step - loss: 3.5284 - acc: 0.2938\n",
      "Epoch 22/100\n",
      "75721/75721 [==============================] - 19s 247us/step - loss: 3.5260 - acc: 0.2938\n",
      "Epoch 23/100\n",
      "75721/75721 [==============================] - 19s 248us/step - loss: 3.5230 - acc: 0.29380s - loss: 3.5181 -\n",
      "Epoch 24/100\n",
      "75721/75721 [==============================] - 19s 247us/step - loss: 3.5207 - acc: 0.2934\n",
      "Epoch 25/100\n",
      "75721/75721 [==============================] - 19s 248us/step - loss: 3.5177 - acc: 0.2934\n",
      "Epoch 26/100\n",
      "75721/75721 [==============================] - 19s 250us/step - loss: 3.5156 - acc: 0.2936\n",
      "Epoch 27/100\n",
      "75721/75721 [==============================] - 19s 249us/step - loss: 3.5133 - acc: 0.2930\n",
      "Epoch 28/100\n",
      "75721/75721 [==============================] - 19s 250us/step - loss: 3.5114 - acc: 0.2922\n",
      "Epoch 29/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.5092 - acc: 0.2937\n",
      "Epoch 30/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.5073 - acc: 0.2929\n",
      "Epoch 31/100\n",
      "75721/75721 [==============================] - 19s 252us/step - loss: 3.5049 - acc: 0.2932\n",
      "Epoch 32/100\n",
      "75721/75721 [==============================] - 19s 250us/step - loss: 3.5030 - acc: 0.2936\n",
      "Epoch 33/100\n",
      "75721/75721 [==============================] - 19s 246us/step - loss: 3.5014 - acc: 0.2939\n",
      "Epoch 34/100\n",
      "75721/75721 [==============================] - 19s 244us/step - loss: 3.5000 - acc: 0.2927\n",
      "Epoch 35/100\n",
      "75721/75721 [==============================] - 19s 255us/step - loss: 3.4981 - acc: 0.2930\n",
      "Epoch 36/100\n",
      "75721/75721 [==============================] - 20s 263us/step - loss: 3.4964 - acc: 0.2931\n",
      "Epoch 37/100\n",
      "75721/75721 [==============================] - 20s 266us/step - loss: 3.4949 - acc: 0.2929\n",
      "Epoch 38/100\n",
      "75721/75721 [==============================] - 20s 260us/step - loss: 3.4933 - acc: 0.2931\n",
      "Epoch 39/100\n",
      "75721/75721 [==============================] - 20s 269us/step - loss: 3.4917 - acc: 0.2929\n",
      "Epoch 40/100\n",
      "75721/75721 [==============================] - 20s 259us/step - loss: 3.4904 - acc: 0.2931\n",
      "Epoch 41/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4892 - acc: 0.2934\n",
      "Epoch 42/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4875 - acc: 0.2930\n",
      "Epoch 43/100\n",
      "75721/75721 [==============================] - 19s 252us/step - loss: 3.4865 - acc: 0.2931\n",
      "Epoch 44/100\n",
      "75721/75721 [==============================] - 19s 252us/step - loss: 3.4848 - acc: 0.2932\n",
      "Epoch 45/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.4838 - acc: 0.2939\n",
      "Epoch 46/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4825 - acc: 0.2933\n",
      "Epoch 47/100\n",
      "75721/75721 [==============================] - 19s 252us/step - loss: 3.4814 - acc: 0.2929\n",
      "Epoch 48/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.4801 - acc: 0.2934\n",
      "Epoch 49/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.4792 - acc: 0.29370s - loss: 3.4795 - acc: 0.\n",
      "Epoch 50/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.4779 - acc: 0.2936\n",
      "Epoch 51/100\n",
      "75721/75721 [==============================] - 19s 255us/step - loss: 3.4769 - acc: 0.2933\n",
      "Epoch 52/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.4760 - acc: 0.2935\n",
      "Epoch 53/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.4750 - acc: 0.2927\n",
      "Epoch 54/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.4742 - acc: 0.2926\n",
      "Epoch 55/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.4733 - acc: 0.2940\n",
      "Epoch 56/100\n",
      "75721/75721 [==============================] - 19s 249us/step - loss: 3.4720 - acc: 0.2935\n",
      "Epoch 57/100\n",
      "75721/75721 [==============================] - 19s 252us/step - loss: 3.4706 - acc: 0.2937\n",
      "Epoch 58/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.4702 - acc: 0.2928\n",
      "Epoch 59/100\n",
      "75721/75721 [==============================] - 19s 250us/step - loss: 3.4695 - acc: 0.2933\n",
      "Epoch 60/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.4685 - acc: 0.2930\n",
      "Epoch 61/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.4679 - acc: 0.2937\n",
      "Epoch 62/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.4671 - acc: 0.2939\n",
      "Epoch 63/100\n",
      "75721/75721 [==============================] - 19s 252us/step - loss: 3.4660 - acc: 0.2933\n",
      "Epoch 64/100\n",
      "75721/75721 [==============================] - 20s 263us/step - loss: 3.4656 - acc: 0.2935\n",
      "Epoch 65/100\n",
      "75721/75721 [==============================] - 19s 252us/step - loss: 3.4649 - acc: 0.2933\n",
      "Epoch 66/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.4637 - acc: 0.2935\n",
      "Epoch 67/100\n",
      "75721/75721 [==============================] - 19s 255us/step - loss: 3.4630 - acc: 0.2934\n",
      "Epoch 68/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.4624 - acc: 0.2926\n",
      "Epoch 69/100\n",
      "75721/75721 [==============================] - 19s 249us/step - loss: 3.4613 - acc: 0.2934\n",
      "Epoch 70/100\n",
      "75721/75721 [==============================] - 19s 251us/step - loss: 3.4609 - acc: 0.2934\n",
      "Epoch 71/100\n",
      "75721/75721 [==============================] - 20s 262us/step - loss: 3.4605 - acc: 0.2934\n",
      "Epoch 72/100\n",
      "75721/75721 [==============================] - 20s 266us/step - loss: 3.4598 - acc: 0.2935\n",
      "Epoch 73/100\n",
      "75721/75721 [==============================] - 20s 259us/step - loss: 3.4588 - acc: 0.2936\n",
      "Epoch 74/100\n",
      "75721/75721 [==============================] - 20s 261us/step - loss: 3.4585 - acc: 0.2939\n",
      "Epoch 75/100\n",
      "75721/75721 [==============================] - 20s 262us/step - loss: 3.4574 - acc: 0.2937\n",
      "Epoch 76/100\n",
      "75721/75721 [==============================] - 20s 265us/step - loss: 3.4572 - acc: 0.2933\n",
      "Epoch 77/100\n",
      "75721/75721 [==============================] - 20s 260us/step - loss: 3.4565 - acc: 0.2936\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75721/75721 [==============================] - 19s 256us/step - loss: 3.4561 - acc: 0.2937\n",
      "Epoch 79/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4556 - acc: 0.2934\n",
      "Epoch 80/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4547 - acc: 0.2937\n",
      "Epoch 81/100\n",
      "75721/75721 [==============================] - 19s 257us/step - loss: 3.4541 - acc: 0.29290s - loss: 3.4524 - \n",
      "Epoch 82/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4535 - acc: 0.2931\n",
      "Epoch 83/100\n",
      "75721/75721 [==============================] - 19s 257us/step - loss: 3.4527 - acc: 0.2939\n",
      "Epoch 84/100\n",
      "75721/75721 [==============================] - 20s 258us/step - loss: 3.4525 - acc: 0.2942\n",
      "Epoch 85/100\n",
      "75721/75721 [==============================] - 20s 258us/step - loss: 3.4519 - acc: 0.2936\n",
      "Epoch 86/100\n",
      "75721/75721 [==============================] - 19s 255us/step - loss: 3.4515 - acc: 0.2929\n",
      "Epoch 87/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4510 - acc: 0.2936\n",
      "Epoch 88/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4501 - acc: 0.2935\n",
      "Epoch 89/100\n",
      "75721/75721 [==============================] - 19s 255us/step - loss: 3.4497 - acc: 0.2936\n",
      "Epoch 90/100\n",
      "75721/75721 [==============================] - 19s 255us/step - loss: 3.4494 - acc: 0.2935\n",
      "Epoch 91/100\n",
      "75721/75721 [==============================] - 19s 255us/step - loss: 3.4493 - acc: 0.2941\n",
      "Epoch 92/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4483 - acc: 0.2930\n",
      "Epoch 93/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.4479 - acc: 0.2938\n",
      "Epoch 94/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.4475 - acc: 0.2941\n",
      "Epoch 95/100\n",
      "75721/75721 [==============================] - 19s 257us/step - loss: 3.4471 - acc: 0.2937\n",
      "Epoch 96/100\n",
      "75721/75721 [==============================] - 19s 257us/step - loss: 3.4465 - acc: 0.2933\n",
      "Epoch 97/100\n",
      "75721/75721 [==============================] - 19s 253us/step - loss: 3.4463 - acc: 0.2933\n",
      "Epoch 98/100\n",
      "75721/75721 [==============================] - 19s 252us/step - loss: 3.4459 - acc: 0.2931\n",
      "Epoch 99/100\n",
      "75721/75721 [==============================] - 19s 254us/step - loss: 3.4455 - acc: 0.2929\n",
      "Epoch 100/100\n",
      "75721/75721 [==============================] - 19s 255us/step - loss: 3.4450 - acc: 0.2937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x262d932d400>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam()\n",
    "one_word_model.compile(optimizer = optimizer, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "one_word_model.fit(one_word_X, one_word_y, epochs = 100, batch_size=128, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how about a prediction function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new(model, tokenizer, seed_text, max_words = 10, sample = True):\n",
    "    ''' generate new text from a trained neural language model '''\n",
    "    seed_word, result = seed_text, seed_text\n",
    "    decoder = {}\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        decoder[index] = word\n",
    "    for _ in range(max_words):\n",
    "        encode = tokenizer.texts_to_sequences([seed_word])[0]\n",
    "        encode = np.array(encode)\n",
    "        y_probs = model.predict(encode)\n",
    "            \n",
    "        if sample:\n",
    "            y_pred = draw_pred(y_probs)\n",
    "        else:\n",
    "            y_pred = np.argmax(y_probs)\n",
    "        \n",
    "        new_word = decoder[y_pred]\n",
    "        \n",
    "        seed_word, result = new_word, result + ' ' + new_word\n",
    "        if new_word == \"eov\":\n",
    "            break\n",
    "        \n",
    "    return result\n",
    "        \n",
    "def draw_pred(probs):\n",
    "    ''' return the sampled index from an array of probs '''\n",
    "    true_probs = probs[0]\n",
    "    draw = np.random.choice(range(len(true_probs)), p = true_probs)\n",
    "    return draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_this = generate_new(one_word_model, tokenizer, \"girl\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girl from a had come back have heads \n",
      "havent packed much distorted never really mean \n",
      "moving all check out \n",
      "shes got for what division \n",
      "and straddle the choice \n",
      "it while the actions are you drink \n",
      "when youre making me \n",
      "assigns a very far too\n"
     ]
    }
   ],
   "source": [
    "print(try_this.replace(\"eol \",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_model.save('one_word_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "sum(pvals[:-1]) > 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-9837059a2d2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtry1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: sum(pvals[:-1]) > 1.0"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-3e0f86ae8a75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;34m\"lucy\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gbv]",
   "language": "python",
   "name": "conda-env-gbv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
